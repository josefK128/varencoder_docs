$$$Animation Synthesis Triggered by Vocal Mimics
$$$https://arxiv.org/pdf/1910.08462.pdf

We propose a method leveraging the naturally time-related expressivity of our voice to control an animation composed of a set
of short events. The user records itself mimicking onomatopoeia
sounds such as "Tick", "Pop", or "Chhh" which are associated with
specific animation events. The recorded soundtrack is automatically analyzed to extract every instance and type of sounds. We
finally synthesize an animation where each event type and timing
correspond with the soundtrack. In addition to being a natural way
to control animation timing, we demonstrate that multiple stories
can be efficiently generated by recording different voice sequences.
Also, the use of more than one soundtrack allows us to control
different characters with overlapping actions.

***

Computer animation generation is an essential tool for entertainment industries such as animation studios or video games developers. With the exception of complex dynamic phenomenons requiring physically based simulations, the fundamental principle
of computer animation for virtual characters is mostly based on
key-framing, meaning that a model should match a predefined
shape at specific times, while in-betweens can be automatically
computed using interpolation schemes. Defining these key-times is
an important step and is called timing by animators

***

In this work, we propose to take advantage of the natural timerelated expressivity of our voice to control animation timing without the use of any manual space-time curve definition. More precisely, a user records a sequence containing different expressive
sounds such as onomatopoeias ("Boum", "Zap", "Bang", etc), acting
as triggers for basic actions constituting the entire animation. Thus,
the timing of the recorded sound sequence defines the timing of
the animation. 
