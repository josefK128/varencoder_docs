{0: 'In this article we revisit the concept of abstraction as it is used in visualization and put it on a solid formal footing. While the term abstraction is utilized in many scientific disciplines, arts, as well as everyday life, visualization inherits the notion of data abstraction or class abstraction from computer science, topological abstraction from mathematics, and visual abstraction from arts. All these notions have a lot in common, yet there is a major discrepancy in the terminology and basic understanding about visual abstraction in the context of visualization. We thus root the notion of abstraction in the philosophy of science, clarify the basic terminology, and provide crisp definitions of visual abstraction as a process. Furthermore, we clarify how it relates to similar terms often used interchangeably in the field of visualization. Visual abstraction is characterized by a conceptual space where this process exists, by the purpose it should serve, and by the perceptual and cognitive qualities of the beholder. These characteristics can be used to control the process of visual abstraction to produce effective and informative visual representations. ', 1: 'The term abstraction often lacks a precise definition in many fields. While several fields have defined the term for their own purposes, there is only a vague understanding of its meaning that is shared by all fields. Some scientific disciplines and scholarly fields have adjusted the vaguely understood meaning to fit the needs of the respective discipline or field. In this article we first present our key definitions related to visual abstraction, and we then provide the justification for the definitions. ', 2: 'An abstraction is a process that transforms a source thing into a less concrete sign thing of the source thing. Abstraction uses a concept of point-of-view, which determines which aspects of source thing should be preserved in its sign thing and which should be suppressed. ', 3: 'A data representation is a sign thing that stands in digital form for a referent thing from reality or another sign thing, using data structures or concept things. Similarly, a visual representation is a sign thing that stands for a referent from reality or another sign thing so that it can be visually perceived and cognitively processed by a human observer. ', 4: 'A visualization is a process that transforms data representations of a thing from reality into visual representations. Visualization is a process that is intended to be a meaningful visual abstraction process. The designers of visualization processes must understand the point-of-view component and tasks. Otherwise, they would not reach the full meaningfulness intended. ', 5: 'In computer science, the term abstraction achieves yet another flavor of its meaning. In object-oriented design, the most frequently used programming methodology, it primarily relates to the definition of classes and methods that cannot be instantiated. Typically, classes and methods are hierarchically grouped into increasingly abstract constructs such that implementations of particular functionality can be shared among many different elements. While for most of these classes it is possible to create instances, an abstract class is a construct that itself cannot be instantiated but which organizes the functionality into a comprehensive representation. The class hierarchy as the outcome of such abstraction gives a clear understanding of differences in functionality among various classes as well as what they have in common. It also facilitates further extensibility of existing code to support new cases that were not considered in the initial software design. ', 6: 'The intuitive understanding of abstraction has been reinforced by this brief excursion into various fields and that stand and argue for abstraction. We can observe that the term is not used uniformly and that it is frequently exchanged with other terms. The recurrent pattern is that abstraction relates to formation of some higher-order constructs or representations that are result of a transformation of lower-level entities. The lowest entities are more tangible, while the higher levels of the abstraction hierarchy are further removed from tangibility and become more mental constructs and concepts  that, in one way or another, allow humans to recognize certain characteristics clearer than the lower-level representations. The ability to abstract seems to be one of the core properties of humans, present while shaping the entire body of analytical knowledge humankind has formed throughout our history. ', 7: 'Automatic generation of a high-quality video from a single image remains a challenging task despite the recent advances in deep generative models. This paper proposes a method that can create a high-resolution, long-term animation using convolutional neural networks (CNNs) from a single landscape image where we mainly focus on skies and waters. Our key observation is that the motion (e. g. , moving clouds) and appearance (e. g. , time-varying colors in the sky) in natural scenes have different time scales. We thus learn them separately and predict them with decoupled control while handling future uncertainty in both predictions by introducing latent codes. ', 8: 'Unlike previous methods that infer output frames directly, our CNNs predict spatially-smooth intermediate data, i. e. , for motion, flow fields for warping, and for appearance, color transfer maps, via self-supervised learning, i. e. , without explicitly-provided ground truth. These intermediate data are applied not to each previous output frame, but to the input image only once for each output frame. This design is crucial to alleviate error accumulation in long-term predictions, which is the essential problem in previous recurrent approaches. The output frames can be looped like cinemagraph, and also be controlled directly by specifying latent codes or indirectly via visual annotations. ', 9: 'To combat the uncertainty of future prediction, we also extract latent codes both for motion and appearance, which depict potential future variations and enable the learning of one-to-many mappings. The user can manipulate the latent codes to control the motion and appearance smoothly in the latent space. Note that the backward flow fields, color transfer functions, and latent codes are learned in a self-supervised manner because their ground-truth data are not available in general. ', 10: 'We propose a method leveraging the naturally time-related expressivity of our voice to control an animation composed of a set of short events. The user records itself mimicking onomatopoeia sounds such as "Tick", "Pop", or "Chhh" which are associated with specific animation events. The recorded soundtrack is automatically analyzed to extract every instance and type of sounds. We finally synthesize an animation where each event type and timing correspond with the soundtrack. In addition to being a natural way to control animation timing, we demonstrate that multiple stories can be efficiently generated by recording different voice sequences. Also, the use of more than one soundtrack allows us to control different characters with overlapping actions. ', 11: 'Computer animation generation is an essential tool for entertainment industries such as animation studios or video games developers. With the exception of complex dynamic phenomenons requiring physically based simulations, the fundamental principle of computer animation for virtual characters is mostly based on key-framing, meaning that a model should match a predefined shape at specific times, while in-betweens can be automatically computed using interpolation schemes. Defining these key-times is an important step and is called timing by animators. ', 12: 'In this work, we propose to take advantage of the natural time related expressivity of our voice to control animation timing without the use of any manual space-time curve definition. More precisely, a user records a sequence containing different expressive sounds such as onomatopoeias ("Boum", "Zap", "Bang", etc), acting as triggers for basic actions constituting the entire animation. Thus, the timing of the recorded sound sequence defines the timing of the animation. ', 13: 'We propose a new form of human-machine interaction. It is a pictorial game consisting of interactive rounds of creation between artists and a machine. They repetitively paint one after the other. At its rounds, the computer partially completes the drawing using machine learning algorithms, and projects its additions directly on the canvas, which the artists are free to insert or modify. Alongside fostering creativity, the process is designed to question the growing interaction between humans and machines. ', 14: 'With the on-going technological revolution, the human-machine interaction is deeply evolving. Hence art creation could benefit of new tools while simultaneously supporting thoughts of how these interactions are affecting humans. Recently, GANs put a spotlight on the creative power of neural networks. For instance were able to generate aesthetic full-stack painting. Yet in these, humans are either engineers or curators. ', 15: 'In this work, we propose a new utilisation of the machine, integrating it at the core of a human creative process. The idea is to suggest to humans, while painting, ramifications and directions of their on-going artwork. In the following, we approach this generic idea under a specific interactive framework. ', 16: 'The artists found the machine strokes surprising and suggestive of move they would not have done by themselves. Actually, some painters have expressed how evocative unintended strokes could be. Our installation where the machine projects completions without painting, combined with generative network capability, allows to explore that in a principled way. Furthermore, the ability to change parameters, such as the learning data set or the amount of completion, adds more degree for the human to control their use of the machine. ', 17: 'From an outside perspective, the machine distorts their original painting style, both on the short term artworks resulting from their interaction (see Figure 2), and on their long term body of work as it inspired them on their machine-free paintings. As such, the interaction is not innocuous, even though, contrarily to our daily experience, we have made the machine impact as explicit as possible with its recognizable blue contributions. ', 18: 'We present a system to help designers create icons that are widely used in banners, signboards, billboards, homepages, and mobile apps. Designers are tasked with drawing contours, whereas our system colorizes contours in different styles. This goal is achieved by training a dual conditional generative adversarial network (GAN) on our collected icon dataset. ', 19: 'One condition requires the generated image and the drawn contour to possess a similar contour, while the other anticipates the image and the referenced icon to be similar in color style. Accordingly, the generator takes a contour image and a man-made icon image to colorize the contour, and then the discriminators determine whether the result fulfills the two conditions. ', 20: 'The trained network is able to colorize icons demanded by designers and greatly reduces their workload. For the evaluation, we compared our dual conditional GAN to several state-of-the-art techniques. Experiment results demonstrate that our network is over the previous networks. ', 21: 'Nowadays, icons are widely utilized in banners, signboards, billboards, homepages, and mobile apps. Effective icons are usually simple but distinguishable, so that users can quickly receive the intended information when seeing them at a small size or a long distance. ', 22: 'Considering aesthetics and practical issues, designing an eye-catching icon is challenging. Designers have to carefully consider not only shapes and structures, but also colors, when they create icons for their customers. ', 23: 'Generative adversarial networks (GANs) have been proven to be able to generate realistic images in many applications and could constitute a solution to help designers colorize icons. Specifically, a network takes a contour image drawn by the designers as input and then outputs the colorized icon image. ', 24: 'To control the colorization process, additional inputs, such as stroke colors and style images, are fed into the network as well. The features extracted from both contour and style images will be fused and used for the colorization. ', 25: 'Observing that an icon can be well defined by color and structure conditions, we present a dual conditional GAN to colorize icons. Rather than training a discriminator to recognize whether an icon is man-made or machine-generated, we train two discriminators to determine whether paired images are similar in structure and color. ', 26: 'We represent the structure condition by a binary contour image. White and black pixels in the image indicate edge and non-edge regions, respectively. To obtain this contour image, the Canny edge detection algorithm is adopted. Intuitively, each icon and its corresponding contour can match, and otherwise cannot. ', 27: 'Since applying a referenced icon to specify the color condition is not intuitive, we let users simply select a style label when using our system to create icons. Specifically, we consider the color psychology theory and define the style of a man-made icon according to its color combination. ', 28: 'We propose a learning based method for generating new animations of a cartoon character given a few example images. Our method is designed to learn from a traditionally animated sequence, where each frame is drawn by an artist, and thus the input images lack any common structure, correspondences, or labels. We express pose changes as a deformation of a layered 2. 5D template mesh, and devise a novel architecture that learns to predict mesh deformations matching the template to a target image. This enables us to extract a common low-dimensional structure from a diverse set of character poses. ', 29: 'we propose a method that learns to generate novel character appearances from a small number of examples by relying on additional user input: a deformable puppet template. We assume that all character poses can be generated by warping the deformable template, and thus develop a deformation network that encodes an image and decodes deformation parameters of the template. These parameters are further used in a differentiable rendering layer that is expected to render an image that matches the input frame. ', 30: 'Some recent work suggests that neural networks can jointly learn the template parameterization and optimize for the alignment between the template and a 3D shape or 2D images. While these models can make inferences over unlabeled data, they are trained on a large number of examples with rich labels, such as dense or sparse correspondences defined for all pairs of training examples. ', 31: 'We propose Neural Turtle Graphics, a novel generative model for spatial graphs, and demonstrate its applications in modeling city road layouts. Specifically, we represent the road layout using a graph where nodes in the graph represent control points and edges in the graph represents road segments. Neural Turtle graphics is a sequential generative model parameterized by a neural network. It iteratively generates a new node and an edge connecting to an existing node conditioned on the current graph. ', 32: 'City road layout modeling is an important problem with applications in various fields. In urban planning, extensive simulation of city layouts are required for ensuring that the final construction leads to effective traffic flow and connectivity. ', 33: 'Graph generation with neural networks has only recently gained attention uses an Recursive Neural network to generate a graph as a sequence of nodes sorted by breadth-first order. It also predicts edges to previous nodes as the new node is added. Producing valid geometry and topology makes our problem particularly challenging. ', 34: 'based on position and normal guidance Given a point cloud our algorithm is able to recognize multiscale ridge-valley features by judging the distance from the current point to the curvature extreme point. At the same time, it can effectively enhance features by the position and normal constraints. Different forms of the constraint can produce different enhancement effects, such as maintaining the original features as much as possible or creating sharper features. ', 35: 'Ridge-valley features are important elements in point clouds that can intuitively sketch the basic shape of objects. These features have important applications in fields including surface reconstruction , surface editing, visual perception and multi-perspective data registering. In general, point clouds are obtained by digitizing real objects with vision measuring equipment. During this process, factors such as object surface reflection and sensor quantization error will affect the accuracy of the data acquisition and weaken the sharpness of the original features. In addition, subsequent processing, such as resampling and smooth filtering, may further damage these features. The effective recognition and enhancement of ridge-valley features are important topics in point cloud processing. ', 36: 'the main contributions of this paper are as follows. A simple and effective ridge-valley point. A parametric surface to fit variations of local surface. The use of extreme point distance criterion to determine feature points. ', 37: 'Autoencoders Rendering realistic images with Global Illumination (GI) is a computationally demanding task and often requires dedicated hardware for feasible runtime. Recent projects have used Generative Adversarial Networks (GAN) to predict indirect lighting on an image level, but are limited to diffuse materials and require training on each scene. We present One-Shot Radiance (OSR), a novel machine learning technique for rendering Global Illumination using Convolutional Autoencoders. We combine a modern denoising Neural Network with Radiance Caching to offer high performance CPU GI rendering while supporting a wide range of material types, without the requirement of offline pre-computation or training for each scene. OSR has been evaluated on interior scenes, and is able to produce high-quality images within 180 seconds on a single CPU. ', 38: 'Ray Tracing is capable of producing photo-realistic images virtually indistinguishable from real pictures. Progressive refinements on rendering algorithms, such as Bi-Directional Path tracing [LW93] and Metropolis Light Transport [VG97] have increased the efficiency of rendering engines in scenarios in which light paths are difficult to evaluate due to the high amount of indirect lighting and Global Illumination (GI). Complex lighting conditions are however still highly expensive to resolve, and most algorithms require long rendering times to reduce the noise from Monte Carlo sampling. ', 39: 'Biased methods have been implemented to produce convincing quality images at a fraction of the cost required by a ray tracer. An early method, Instant Radiosity [Kel97], exploited the low rate of illumination change over diffuse surfaces to approximate GI by rendering the same scene many times using Virtual Point Lights sampled at locations reached by the main light sources to simulate secondary bounces. ', 40: 'The most immediate way to apply Machine Learning to graphics and ray tracing is to operate on the final rendered image level. These approaches start by taking as input a scene rendered from the final camera’s viewpoint. They then attempt to output a transformation that results in higher visual quality, removal of noise, or addition of effects. ', 41: 'Global illumination with radiance regression functions [RWG* 13] focuses on realtime indirect illumination rendering using a neural network that learns the relationship between local and contextual attributes such as vertices and light position, to the indirect illumination value. This method shows very good performance and quality, although it is limited to point light sources and requires pre-baking of the Radiance Regression Function for each scene. ', 42: 'We present a 3D stylization algorithm that can turn an input shape into the style of a cube while maintaining the content of the original shape. The key insight is that cubic style sculptures can be captured by the as rigid as possible energy with a regularization on rotated surface normals. Minimizing this energy naturally leads to a detail preserving cubic geometry. Our optimization can be solved efficiently without any mesh surgery. Our method serves as a non-realistic modeling tool where one can incorporate many artistic controls to create stylized geometries. ', 43: 'The availability of image stylization filters and non-photorealistic rendering techniques has dramatically lowered the barrier of creating artistic imagery to the point that even a non-professional user can easily create stylized images. In stark contrast, direct stylization of 3D shapes or non-realistic modeling has received far less attention. In professional industries such as visual effects and video games, trained modelers are still required to meticulously create non-realistic geometric assets. This is because investigating geometric styles is more challenging due to arbitrary topologies, curved metrics, and non-uniform discretization. The scarcity of tools to generate artistic geometry remains a major roadblock to the development of geometric stylization. ', 44: 'GAN art often exhibits visual indeterminacy. GANs cause visual indeterminacy by creating plausible compositions and textures that nonetheless defy coherent explanation, and these are the GAN images often used in recent artworks. Because visual indeterminacy can be understood as a perceptual process, GANs provide a potential tool for both art and for neuroscience experiments based on perceptual uncertainty modeling. ', 45: 'Often, the initial appearance of an image invites the viewer to investigate further, but the image confounds explanation. For some images, this investigation leads to an “Aha!” moment, where the viewer understands the structure of an image. They see a vivid 3D object where there had been abstract 2D shapes. This moment is pleasurable because the posterior distribution collapses. Some understanding has been gained. But the image may also become less interesting as result. ', 46: 'Natural image models, vision neuroscience, and image synthesis have long been tightly-coupled fields. Discoveries about the visual cortex led to natural image statistics analysis, which led to texture synthesis algorithms which led to style transfer algorithms. At the same time, cortical modeling also led to deep convolution networks, which led to GANs and trained discriminative networks, which, in turn, have led to improved neuroscience models. Can aspects of aesthetic experience be understood with the same models?. ', 47: 'Ideally, a generator would accurately represent a distribution over natural images; a recognition model would be the inverse, providing a posterior distribution over interpretations that would approximate human perceptual uncertainty. Optimization against this model would give artists more precise control over the type of perceptual uncertainty present in images, for example, to produce images with specific types of visual indeterminacy. ', 48: 'This would provide artists with higher-level controls to explore artistic creation. It could also provide a richer testbed to develop perceptual theories of aesthetic experience, rather than using hand-crafted artwork. ', 49: 'A more fine-grained neural model of indeterminacy is needed, to account for the temporal evolution from first impressions [14], to the movement of attention, to the possibility of the “Aha!” moment. Moreover, the categorization of perceptual ambiguity in art is very preliminary and much work remains to be done to expand upon and refine it. '}